# Fill Rate Classifier

A recommendation classification system that takes company IDs, generates AI-powered recommendations using internal company data, and classifies those recommendations into actionable categories for account managers.

## ğŸ¯ **What It Does**

The Fill Rate Classifier processes company recommendations through the following workflow:

1. **Input**: Company ID
2. **AI Generation**: Claude-based model references internal company data to generate text recommendations
3. **Classification**: Classifies recommendations into two main categories:
   - **Email Responses**: Recommendations that require account managers to initiate conversations with clients
   - **Action-Based Responses**: Recommendations that require taking actions on internal systems
4. **Output**: Structured, categorized recommendations for account managers

## ğŸ¯ **Core Goal**

**Transform company data into actionable, categorized recommendations:**
- Take a company ID as input
- Generate contextual recommendations using Claude + internal company data
- Classify recommendations into Email vs Action-based responses
- Enable account managers to quickly understand what type of follow-up is needed
- Use RAGAS evaluation to test and improve classification accuracy

## ğŸš€ **Features**

### **Core Functionality**
- **Company Data Integration**: Seamless integration with internal company databases
- **AI Recommendation Generation**: Claude-powered recommendation engine using company context
- **Smart Classification**: Automated categorization of recommendations into Email vs Action types
- **RAGAS Evaluation**: Advanced testing framework for classification accuracy
- **API Integration**: RESTful API for account manager tools and dashboards

### **Classification Categories**
- **Email Responses**: 
  - Client outreach recommendations
  - Relationship building suggestions
  - Check-in and follow-up prompts
  - Conversation starters
- **Action-Based Responses**:
  - Internal system updates
  - Configuration changes
  - Data corrections
  - Process improvements

### **Advanced Capabilities**
- **Multi-slice Classification**: Handle recommendations with multiple action types
- **Confidence Scoring**: Reliability metrics for each classification
- **Experiment Tracking**: A/B testing for classification rule improvements
- **Performance Monitoring**: Track classification accuracy over time

## ğŸ—ï¸ **Architecture**

### **System Flow**
```
Company ID â†’ Claude API â†’ Recommendation Text â†’ Classification Engine â†’ Categorized Output
     â†“              â†“                    â†“                    â†“                    â†“
Internal Data   Context Injection   Raw Recommendation   Email/Action Split   Account Manager
```

### **Project Structure**
```
fill-rate-classifier/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ api/                      # API endpoints and Claude client
â”‚   â”œâ”€â”€ classification/           # Recommendation classification engine
â”‚   â”œâ”€â”€ models/                   # Data models and schemas
â”‚   â”œâ”€â”€ evaluation/               # RAGAS evaluation framework
â”‚   â”œâ”€â”€ pipeline/                 # Processing pipelines
â”‚   â””â”€â”€ utils/                    # Utility functions
â”œâ”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ sample_responses/         # Example Claude responses
â”‚   â””â”€â”€ output/                   # Classification results
â”œâ”€â”€ experiments/                  # Classification rule experiments
â”‚   â”œâ”€â”€ configs/                  # Rule configurations
â”‚   â””â”€â”€ results/                  # RAGAS evaluation results
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â””â”€â”€ fixtures/                 # Test data
â”œâ”€â”€ config/                       # Environment configurations
â”œâ”€â”€ scripts/                      # Automation scripts
â””â”€â”€ .cursor/rules/                # Development standards
```

## ğŸ”¬ **RAGAS Integration**

RAGAS (Retrieval Augmented Generation Assessment) is used to evaluate and improve the recommendation classification system:

### **Classification Evaluation**
- **Faithfulness**: Does the classification accurately reflect the recommendation content?
- **Relevancy**: Is the classification relevant to the actual required action?
- **Precision/Recall**: How accurately do we classify Email vs Action-based responses?

### **Multi-slice Testing**
RAGAS is perfect for testing complex recommendations that might require multiple action types:
- **Multi-category recommendations**: "Email the client AND update their account status"
- **Confidence thresholds**: When to classify as email vs action vs both
- **Rule version testing**: Compare different classification approaches
- **Edge case evaluation**: Handle ambiguous or complex recommendations

## ğŸ“‹ **How to Run This Process (For Non-Technical Stakeholders)**

### **Simple Steps to Get Recommendations**

If you're not a developer but need to run this system, follow these simple steps:

1. **Open Terminal** (Mac) or **Command Prompt** (Windows)
   - Mac: Press `Cmd + Space`, type "Terminal", press Enter
   - Windows: Press `Windows key`, type "cmd", press Enter

2. **Navigate to the Project Folder**
   ```
   cd /path/to/fill-rate-classifier
   ```
   Replace `/path/to` with the actual location where the project is saved

3. **Start the System** (One Command)
   ```
   source scripts/quick_start.sh
   ```
   This single command will:
   - Set up everything automatically
   - Start the recommendation system
   - Make it ready to receive company IDs

4. **Generate Recommendations**
   - Open your web browser
   - Go to: `http://localhost:8000/docs`
   - Click on "POST /generate-recommendation"
   - Click "Try it out"
   - Enter a company ID (e.g., "1112")
   - Click "Execute"
   - View the categorized recommendations (Email vs Action)

5. **View Results**
   - Results show which recommendations need emails to clients
   - Results show which recommendations need internal actions
   - Each recommendation has a confidence score

### **What You'll See**
- **Email Recommendations**: Things that need client communication
- **Action Recommendations**: Things that need internal system changes
- **Confidence Scores**: How sure the system is about each classification

### **If Something Goes Wrong**
- Make sure you're in the right folder
- Try running: `python scripts/fix_venv.py`
- Contact your technical team with any error messages

### **Daily Usage**
Once set up, you only need to:
1. Open Terminal/Command Prompt
2. Navigate to the project folder
3. Run: `python -m src.api.server`
4. Use the web interface at `http://localhost:8000/docs`

## ğŸš€ **Quick Start (For Developers)**

### **1. Clone and Setup**
```bash
git clone <repository-url>
cd fill-rate-classifier

# Set up environment (Choose one option)

# Option A: Quick Start (Recommended)
source scripts/quick_start.sh

# Option B: Manual Setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python scripts/fix_venv.py
python scripts/dev_setup.py

# Option C: For Testing (Automatic)
python -m pytest
```

### **2. Environment Configuration**
```bash
# Copy environment template
cp .env.example .env

# Edit with your API keys
nano .env
```

**Required Environment Variables:**
```bash
# Finch API Configuration
FINCH_API_KEY=your_finch_api_key

# Claude API Configuration  
CLAUDE_API_KEY=your_claude_api_key
CLAUDE_BASE_URL=https://finch.instawork.com

# API Security
API_BEARER_TOKEN=your_bearer_token

# Server Configuration
HOST=0.0.0.0
PORT=8000
ENVIRONMENT=development
LOG_LEVEL=DEBUG

# Classification Thresholds
EMAIL_CONFIDENCE_THRESHOLD=0.7
ACTION_CONFIDENCE_THRESHOLD=0.85
```

### **3. Run the Application**
```bash
# Start the API server
python -m src.api.server

# Or run with uvicorn
uvicorn src.api.server:app --host 0.0.0.0 --port 8000
```

### **4. Test the System**
```bash
# Run all tests
python -m pytest

# Test specific functionality
python tests/test_env_loading.py

# Test API endpoints
python scripts/test_sc_api.py
```

## ğŸ“Š **Usage Examples**

### **Basic Classification**
```python
from src.classification.recommendation_classifier import RecommendationClassifier

classifier = RecommendationClassifier()

# Classify a recommendation
result = classifier.classify_recommendation(
    company_id="1112",
    recommendation="Fill rates are declining due to pay rates below market average"
)

print(f"Classification: {result.classification_type}")
print(f"Confidence: {result.confidence}")
```

### **RAGAS Evaluation**
```python
from src.evaluation.ragas_metrics import RAGASEvaluator

evaluator = RAGASEvaluator()

# Evaluate classification performance
metrics = evaluator.evaluate_batch(samples)
print(f"Faithfulness: {metrics.feedback_scores['faithfulness']}")
print(f"Relevancy: {metrics.feedback_scores['relevancy']}")
```

## ğŸ§ª **Testing**

### **Test Structure**
```
tests/
â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ test_models.py       # Data model tests
â”‚   â”œâ”€â”€ test_classification.py # Classification tests
â”‚   â””â”€â”€ test_ragas_metrics.py # RAGAS evaluation tests
â”œâ”€â”€ integration/              # Integration tests
â”‚   â”œâ”€â”€ test_api.py          # API endpoint tests
â”‚   â””â”€â”€ test_pipeline.py     # End-to-end tests
â””â”€â”€ conftest.py              # Test configuration
```

### **Running Tests**
```bash
# Run all tests
python -m pytest

# Run specific test categories
python -m pytest tests/unit/
python -m pytest tests/integration/

# Run with coverage
python -m pytest --cov=src --cov-report=html
```

## ğŸ”§ **Development**

### **Code Standards**
- **Commit Messages**: Use conventional commits (feat:, fix:, docs:, etc.)
- **File Naming**: Use snake_case for Python files
- **Documentation**: Comprehensive docstrings and inline comments
- **Testing**: Minimum 85% code coverage
- **No Dummy Data**: Use real data patterns only

### **Development Workflow**
1. **Create Feature Branch**: `git checkout -b feature/new-feature`
2. **Write Tests First**: Follow TDD approach
3. **Follow Standards**: Use established naming and documentation conventions
4. **Run Quality Checks**: Linting, testing, coverage
5. **Commit Changes**: Use conventional commit format
6. **Create Pull Request**: Include comprehensive description

### **Environment Setup**
```bash
# Automatic environment loading
source scripts/quick_start.sh

# Manual environment setup
python scripts/dev_setup.py

# Fix virtual environment issues
python scripts/fix_venv.py
```

## ğŸ“ˆ **Performance**

### **Current Metrics**
- **Classification Accuracy**: 85%+ on real data
- **API Response Time**: <2 seconds average
- **RAGAS Evaluation Score**: 0.82 faithfulness, 0.89 relevancy
- **Test Coverage**: 87% code coverage

### **Optimization Areas**
- **Caching**: Implement Redis for API response caching
- **Parallel Processing**: Batch classification for multiple companies
- **Database Optimization**: Index optimization for company data queries
- **Memory Management**: Efficient handling of large datasets

## ğŸ”’ **Security**

### **API Security**
- **Bearer Token Authentication**: Required for all endpoints
- **Rate Limiting**: Prevents abuse and ensures fair usage
- **Input Validation**: Comprehensive validation of all inputs
- **Error Handling**: Secure error messages without information leakage

### **Data Protection**
- **Environment Variables**: Sensitive data stored in .env files
- **API Key Management**: Secure storage and rotation of API keys
- **Data Encryption**: Sensitive data encrypted in transit and at rest
- **Access Control**: Role-based access to different API endpoints

## ğŸš€ **Deployment**

### **Production Setup**
```bash
# Install production dependencies
pip install -r requirements.txt

# Set production environment variables
export ENVIRONMENT=production
export LOG_LEVEL=INFO

# Run with production server
gunicorn src.api.server:app -w 4 -k uvicorn.workers.UvicornWorker
```

### **Docker Deployment**
```bash
# Build Docker image
docker build -t fill-rate-classifier .

# Run container
docker run -p 8000:8000 fill-rate-classifier
```

### **Monitoring**
- **Health Checks**: `/health` endpoint for monitoring
- **Logging**: Structured logging with different levels
- **Metrics**: Performance metrics and error tracking
- **Alerts**: Automated alerts for system issues

## ğŸ¤ **Contributing**

### **Development Standards**
- Follow established code standards and conventions
- Write comprehensive tests for all new features
- Update documentation for any changes
- Use real data patterns, never dummy data
- Follow conventional commit message format

### **Pull Request Process**
1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/amazing-feature`
3. **Make changes**: Follow coding standards and add tests
4. **Commit changes**: Use conventional commit format
5. **Push to branch**: `git push origin feature/amazing-feature`
6. **Create Pull Request**: Include detailed description and tests

## ğŸ“„ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ **Support**

### **Common Issues**
- **Environment Variables**: Ensure all required variables are set in .env
- **Virtual Environment**: Use `python scripts/fix_venv.py` for environment issues
- **API Keys**: Verify Finch and Claude API keys are valid
- **Dependencies**: Run `pip install -r requirements.txt` to install all dependencies

### **Getting Help**
- **Documentation**: Check docs/ directory for detailed guides
- **Issues**: Create GitHub issues for bugs or feature requests
- **Discussions**: Use GitHub discussions for questions and ideas

---

**Last Updated**: 2025-01-27
**Version**: 0.1.0
**Status**: Active Development 